.. _development_pipeline:

=============================
Pipeline
=============================

A common design pattern for projects like PDAL is to provide a set of base
classes which, when connected together, form a processing chain -- that is,
a pipeline.

libLAS leans this way.  Both LizardTech SDKs (raster and lidar) are
explicitly implemented this way.  GDAL doesn't really do it this way.  mpg
likes this model, if only because he's done it before.


An Motivating Example
=====================

Consider the following class hierarchy::

  class Stage
  {
    // returns the next point
    Point readPoint() = 0;
  }

  class LasReader : Stage
  {
    LasReader(string infile);
    Point readPoint();
  }

  class LasWriter
  {
    LasWriter(Stage input, string outfile);
    void writeAll();
  }

  class Decimate : Stage
  {
    Decimate(Stage input, int n); // retains every Nth point
    Point readPoint();
  }

Now, using these pieces we can construct a workflow which reads in a LAS
file and writes out a LAS file that is 90% smaller::

  LasReader reader("in.las");
  Decimate decimator(reader, 10);
  LasWriter writer(decimator, "out.las");
  writer.writeAll();
  
Here are the implementations of Decimate::readPoint() and
LasWriter::writeAll()::
  
  Point Decimate::readPoint()
  {
    Point p;
    
    // read N points, only keep the last one
    for (int i=0; i<m_n; i++)
      p = m_input.readPoint()
      
    return p;
  }
  
  LasWriter::writeAll()
  {
    while (1)
    {
      Point p = m_input.readPoint();
      if (p.isEmpty()) break; // last point
      
      write_to_las_file(p);
    }
  }

This is all very clean and simple: it follows a "pull" model, with the
writer stage requesting data from its previous stage, which in turns
requests (pulls) data from its previous stage.

The tricky bit, of course, is making it all run efficiently.


Where It Starts to Get All Tricksy
==================================

In the above example, at each stage all the points are (presumably) copied
from the previous stage into the current stage.  We want to avoid as much
copying as possible, so we need to be clever here.  We also want to be able
to operate on not a single Point but on blocks of Points.

In the MrSID raster SDK, the front stage in the pipeline makes a fixed size
buffer of pixels and then passes a pointer to the buffer to the previous 
stage.  A pixel buffer, when allocated, is defined to hold only a certain
type of pixel -- datatype size, number of bands, etc.  And so depending on
what kind of stage that previous stage is, it can do one of several things:

* Fill in the buffer itself -- this is what as would a Reader would do

* Pass the buffer pointer to its previous stage, call read() on the previous
  stage, and then modify the contents of the buffer -- this is what a filter
  like "add 1 to each pixel" would do
  
* Create a new buffer, pass that to the previous stage, call read() on the
  previous stage, and then copy the contents from that new buffer into the
  original buffer -- this is what a filter like "convert uint16 samples to
  uint8 samples" would do, because the buffers are set up to contain only
  one kind of data at a time.

We used the term "impedance" to refer to whether or not two stages could use
the same buffer or not.  For example, a buffer which stored 8-bit RGB pixel
data would have an "impedance mismatch" if it was passed to a previous stage
that only knew how to handle 16-bit grayscale pixel buffers.  When such a
mismatch was present, logic would need to be present (in one of the stages,
or in a new inserted stage) to do the conversion operation from one buffer
type to the other.

Now, understand this was not a runtime issue -- the programmer was
responsible for constructing a "valid" pipeline in his code, and if he did
not then a runtime error ("impedance mismatch!") would be returned at
runtime.


Questions for PDAL
===================

How should we handle the buffering issue in PDAL for Points?

Should we use a very static buffering strategy like the above, or something different?

Do we want to follow the model of "static" pipeline construction, or allow
for something more dynamic, such as automagic insertion of stages that would
do the "compensation code" for you?

Howard said (I think) that he wanted to have stages be able to "advertise"
what properties or abilities that it supports, such as "I can efficiently
read random points" or "I can return either F32 or F64 data" or "I only
understand XYZ data and will ignore anything else".  How does this idea play
into the more general question of how much smarts go into the pipeline
construction process?


Notes
=====

* three kinds of filtering operations that the point buffer must support:

  - add or remove a point
  
  - add or remove a field
  
  - modify the value in a field